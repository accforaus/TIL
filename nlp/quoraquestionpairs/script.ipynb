{"cells":[{"metadata":{"_cell_guid":"60c40138-3679-419e-b455-0653b65b36bf","_uuid":"dfc9ce7eeb68dfac052ddc766b426938d2668cfb"},"cell_type":"code","outputs":[],"execution_count":null,"source":["from time import time\n","import pandas as pd\n","import numpy as np\n","from gensim.models import KeyedVectors\n","import re\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import itertools\n","import datetime\n","\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Model\n","from keras.layers import Input, Embedding, LSTM, Merge\n","import keras.backend as K\n","from keras.optimizers import Adadelta\n","from keras.callbacks import ModelCheckpoint"]},{"metadata":{"_cell_guid":"fe4ead80-049e-43c6-a405-781d9653ee22","collapsed":true,"_uuid":"d30145c2dfda8a2eee4730b3502899513f51a5e4"},"cell_type":"code","outputs":[],"execution_count":null,"source":["TRAIN_CSV = '../input/quora-question-pairs/train.csv'\n","TEST_CSV = '../input/quora-question-pairs/test.csv'\n","EMBEDDING_FILE = '../input/kowordvec/ko.bin'\n","#MODEL_SAVING_DIR = '/home/ecohen/HDD/HDD4/Models/Kaggle/Quora/'\n","#train_df = pd.read_csv('../input/quora-question-pairs/train.csv')"]},{"metadata":{"_cell_guid":"5d65ee1a-fa58-4fd2-846f-85873d15398c","collapsed":true,"_uuid":"a3cbe53314e18a8b2635c63038ab5e33b2737f6c"},"cell_type":"code","outputs":[],"execution_count":null,"source":["train_df = pd.read_csv(TRAIN_CSV)\n","test_df = pd.read_csv(TEST_CSV)"]},{"metadata":{"_cell_guid":"6241f91b-e0c4-4722-b184-abcb14577d04","_uuid":"df4075a5d6cc415c83e448da616a2fac0ab4df26"},"cell_type":"code","outputs":[],"execution_count":null,"source":["stops = set(stopwords.words('english'))\n","train_df"]},{"metadata":{"_cell_guid":"1773936c-6713-4837-a7a4-54d3125f7232","collapsed":true,"_uuid":"91499b8f1dac249ecfcc03a525030e5810c6e978"},"cell_type":"code","outputs":[],"execution_count":null,"source":["def text_to_word_list(text):\n","    ''' Pre process and convert texts to a list of words '''\n","    text = str(text)\n","    text = text.lower()\n","\n","    # Clean the text\n","    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n","    text = re.sub(r\"what's\", \"what is \", text)\n","    text = re.sub(r\"\\'s\", \" \", text)\n","    text = re.sub(r\"\\'ve\", \" have \", text)\n","    text = re.sub(r\"can't\", \"cannot \", text)\n","    text = re.sub(r\"n't\", \" not \", text)\n","    text = re.sub(r\"i'm\", \"i am \", text)\n","    text = re.sub(r\"\\'re\", \" are \", text)\n","    text = re.sub(r\"\\'d\", \" would \", text)\n","    text = re.sub(r\"\\'ll\", \" will \", text)\n","    text = re.sub(r\",\", \" \", text)\n","    text = re.sub(r\"\\.\", \" \", text)\n","    text = re.sub(r\"!\", \" ! \", text)\n","    text = re.sub(r\"\\/\", \" \", text)\n","    text = re.sub(r\"\\^\", \" ^ \", text)\n","    text = re.sub(r\"\\+\", \" + \", text)\n","    text = re.sub(r\"\\-\", \" - \", text)\n","    text = re.sub(r\"\\=\", \" = \", text)\n","    text = re.sub(r\"'\", \" \", text)\n","    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n","    text = re.sub(r\":\", \" : \", text)\n","    text = re.sub(r\" e g \", \" eg \", text)\n","    text = re.sub(r\" b g \", \" bg \", text)\n","    text = re.sub(r\" u s \", \" american \", text)\n","    text = re.sub(r\"\\0s\", \"0\", text)\n","    text = re.sub(r\" 9 11 \", \"911\", text)\n","    text = re.sub(r\"e - mail\", \"email\", text)\n","    text = re.sub(r\"j k\", \"jk\", text)\n","    text = re.sub(r\"\\s{2,}\", \" \", text)\n","\n","    text = text.split()\n","\n","    return text"]},{"metadata":{"_uuid":"fb2636cb27a621cf40d5d363c52f6e1bd96953b3","_cell_guid":"95abb46e-f541-4d63-afc6-b36ee03b5e09"},"cell_type":"code","outputs":[],"execution_count":null,"source":["vocabulary = dict()\n","inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n","word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True, encoding='utf8',unicode_errors='ignore')\n","# from gensim.models.wrappers import FastText\n","# word2vec = FastText.load_fasttext_format(EMBEDDING_FILE)\n","\n","questions_cols = ['question1', 'question2']"]},{"metadata":{"_cell_guid":"7d6926e3-493f-4ca7-8975-523b9361ba9e","collapsed":true,"_uuid":"efde70d178323c9188ab4c55f7db4c10b9c1c673"},"cell_type":"code","outputs":[],"execution_count":null,"source":["# tokenize\n","c = 0\n","for question in tqdm(questions):\n","    questions[c] = list(gensim.utils.tokenize(question, deacc=True, lower=True))\n","    c += 1\n","print(\"ok\")\n","# train model\n","model = gensim.models.Word2Vec(questions, size=300, workers=-1, iter=10, negative=20)\n","print(\"ok2\")\n","# trim memory\n","model.init_sims(replace=True)"]},{"metadata":{"_cell_guid":"08844323-a9dd-41e7-bc74-ec542ec7d42f","collapsed":true,"_uuid":"ea80d3fe455a985ed07c6b0b4607f8a25f8c1b7d"},"cell_type":"code","outputs":[],"execution_count":null,"source":["questions_cols = ['question1', 'question2']"]},{"metadata":{"_cell_guid":"06e653dc-7bc4-44e5-8f2d-0912db064ba7","collapsed":true,"_uuid":"a522ae196ac950026d64e3cb26aa8b37470661da"},"cell_type":"code","outputs":[],"execution_count":null,"source":["model.init_sims(replace=True)\n","word2vec = model"]},{"metadata":{"_cell_guid":"7515fb12-2978-4c46-86c1-5d6c1c4dd085","collapsed":true,"_uuid":"f5952354234fc7e5768b2dacb9d28380f0f6ca82"},"cell_type":"code","outputs":[],"execution_count":null,"source":["# Iterate over the questions only of both training and test datasets\n","for dataset in [train_df, test_df]:\n","    for index, row in dataset.iterrows():\n","\n","        # Iterate through the text of both questions of the row\n","        for question in questions_cols:\n","\n","            q2n = []  # q2n -> question numbers representation\n","            for word in text_to_word_list(row[question]):\n","\n","                # Check for unwanted words\n","                \n","                if word in stops and word not in word2vec.vocabulary:\n","                    continue\n","\n","                if word not in vocabulary:\n","                    vocabulary[word] = len(inverse_vocabulary)\n","                    q2n.append(len(inverse_vocabulary))\n","                    inverse_vocabulary.append(word)\n","                else:\n","                    q2n.append(vocabulary[word])\n","\n","            # Replace questions as word to question as number representation\n","            dataset.set_value(index, question, q2n)"]},{"metadata":{"_cell_guid":"2aa5cba5-1647-4516-987f-954c36791656","collapsed":true,"_uuid":"87477f4fb4c83b2c0c0a67af1a666e62110f9f6d"},"cell_type":"code","outputs":[],"execution_count":null,"source":["# save model\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","model.save('3_word2vec.mdl')\n","model.wv.save_word2vec_format('3_word2vec.bin', binary=True)"]},{"metadata":{"_cell_guid":"3c9ee56d-6cf4-4d98-8cfd-e22e94f93775","collapsed":true,"_uuid":"36a9e7a0d094790bec4f13146716a7aafb6fd375"},"cell_type":"code","outputs":[],"execution_count":null,"source":["import spacy"]},{"metadata":{"_cell_guid":"d4a06421-b6d5-4b76-b9ac-c143e1d29b21","collapsed":true,"_uuid":"131bae776f2ddd8be43d0e085446b331b5647163"},"cell_type":"code","outputs":[],"execution_count":null,"source":["# exctract word2vec vectors\n","import spacy\n","nlp = spacy.load('en')\n","print(model.wv.get_vector('trump'))\n","\n","vecs1 = [doc.vector for doc in nlp.pipe(train_df['question1'], n_threads=-1, batch_size=20)]\n","vecs1 =  np.array(vecs1)\n","print(\"ok\")\n","train_df['q1_feats'] = list(vecs1)\n","    \n","vecs2 = [doc.vector for doc in nlp.pipe(train_df['question2'], n_threads=-1, batch_size=20)]\n","vecs2 =  np.array(vecs2)\n","train_df['q2_feats'] = list(vecs2)\n","\n","    # save features\n","pd.to_pickle(df, '1_df.pkl')"]},{"metadata":{"_cell_guid":"97511f3c-e259-4278-85e6-065bc64610c3","collapsed":true,"_uuid":"3199086edab2fc870bbe01d4cedf7f66d0ff345e"},"cell_type":"code","outputs":[],"execution_count":null,"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","# merge texts\n","questions = list(train_df['question1']) + list(train_df['question2'])\n","\n","tfidf = TfidfVectorizer(lowercase=False, )\n","tfidf.fit_transform(questions)\n","\n","# dict key:word and value:tf-idf score\n","word2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))"]},{"metadata":{"_cell_guid":"fad23670-60a0-4b94-a544-e57b5c60c441","collapsed":true,"_uuid":"3ae8271d0595798059253b01ca36139ade74bba4"},"cell_type":"code","outputs":[],"execution_count":null,"source":["#word2tfidf\n","#w2v.keys()\n","questions"]},{"metadata":{"_cell_guid":"0395475b-1582-4266-b267-d7721b85bf02","collapsed":true,"_uuid":"77666925ba9df9baca4b751920c8bf34aa844475"},"cell_type":"code","outputs":[],"execution_count":null,"source":["train_df"]},{"metadata":{"_cell_guid":"7dea12f4-c376-4a25-8a69-a6b603b77a14","collapsed":true,"_uuid":"633daf6f832c9951a342e387f00e8c7e53990b2f"},"cell_type":"code","outputs":[],"execution_count":null,"source":["# spacy를 대체함\n","#from konlpy.tag import Kkma"]},{"metadata":{"_cell_guid":"821351cc-7414-4136-ade5-c8ea7487a973","collapsed":true,"_uuid":"750cbe7bcf05bc88504d1ffe631a1f1c40299c08"},"cell_type":"code","outputs":[],"execution_count":null,"source":["#twitter = Twitter()\n","#twitter.pos(\"What is the story of Kohinoor (Koh-i-Noor) Diamond?\")"]},{"metadata":{"_cell_guid":"9e09e83a-d9e7-4b20-a214-91ce23ce7e15","collapsed":true,"_uuid":"c3b458561be05dd552746ce842ee9ee9e930c7f1"},"cell_type":"code","outputs":[],"execution_count":null,"source":["# exctract word2vec vectors\n","#import spacy\n","#nlp = spacy.load('en')\n","\n","vecs1 = []\n","for qu in tqdm(list(train_df['question1'])):\n","    doc = twitter.pos(qu) \n","    mean_vec = np.zeros([len(doc), 300])\n","    for word in doc:\n","        # print(word)\n","        # word2vec\n","        \n","        try:\n","            vec = model.wv.get_vector(word[0])\n","        except KeyError as ex:\n","            #print(ex)\n","            continue\n","            \n","        # fetch df score\n","        \n","        try:\n","            idf = word2tfidf[str(word)]\n","        except:\n","            #print word\n","            idf = 0\n","            \n","        # compute final vec\n","        mean_vec += vec * idf\n","    mean_vec = mean_vec.mean(axis=0)\n","    vecs1.append(mean_vec)\n","train_df['q1_feats'] = list(vecs1)\n","train_df['q1_feats']"]},{"metadata":{"_cell_guid":"40bcc466-8208-4f8b-b8f9-bcd19ae44cb8","collapsed":true,"_uuid":"f31b84dbdb0a582948c62e8cd5bdc19c5dee58cc"},"cell_type":"code","outputs":[],"execution_count":null,"source":["\n","vecs2 = []\n","for qu in tqdm(list(train_df['question2'])):\n","    doc = twitter.pos(qu) \n","    mean_vec = np.zeros([len(doc), 300])\n","    for word in doc:\n","        # print(word)\n","        # word2vec\n","        \n","        try:\n","            vec = model.wv.get_vector(word[0])\n","        except KeyError as ex:\n","            #print(ex)\n","            continue\n","            \n","        # fetch df score\n","        \n","        try:\n","            idf = word2tfidf[str(word)]\n","        except:\n","            #print word\n","            idf = 0\n","            \n","        # compute final vec\n","        mean_vec += vec * idf\n","    mean_vec = mean_vec.mean(axis=0)\n","    vecs2.append(mean_vec)\n","train_df['q2_feats'] = list(vecs2)\n","train_df['q2_feats']"]},{"metadata":{"_cell_guid":"a07d6928-bbe1-4690-9996-9e4e162c8ad2","collapsed":true,"_uuid":"84089f499765373b8b3f716b79d068740bdb4785"},"cell_type":"code","outputs":[],"execution_count":null,"source":["qu = \"안녕하세요. 딥러닝 초보자입니다\"\n","doc = twitter.pos(qu) \n","train_df.head()"]},{"metadata":{"_cell_guid":"996ef936-225e-45a3-8110-2f942151af1f","collapsed":true,"_uuid":"8c609d36690da8f6bbd0e2ade90bafbe71f9bba0"},"cell_type":"code","outputs":[],"execution_count":null,"source":["doc"]},{"metadata":{"_cell_guid":"fe319555-50a9-4d01-a435-1476442e7dda","collapsed":true,"_uuid":"16b90f6a0932bcf1b41f7651459e3c4d8e17927b"},"cell_type":"code","outputs":[],"execution_count":null,"source":["# shuffle df\n","train_df = train_df.reindex(np.random.permutation(train_df.index))\n","\n","# set number of train and test instances\n","num_train = int(train_df.shape[0] * 0.88)\n","num_test = train_df.shape[0] - num_train                 \n","print(\"Number of training pairs: %i\"%(num_train))\n","print(\"Number of testing pairs: %i\"%(num_test))\n","\n","# init data data arrays\n","X_train = np.zeros([num_train, 2, 300])\n","X_test  = np.zeros([num_test, 2, 300])\n","Y_train = np.zeros([num_train]) \n","Y_test = np.zeros([num_test])\n","\n","# format data \n","b = [a[None,:] for a in list(train_df['q1_feats'].values)]\n","q1_feats = np.concatenate(b, axis=0)\n","\n","b = [a[None,:] for a in list(train_df['q2_feats'].values)]\n","q2_feats = np.concatenate(b, axis=0)\n","\n","# fill data arrays with features\n","X_train[:,0,:] = q1_feats[:num_train]\n","X_train[:,1,:] = q2_feats[:num_train]\n","Y_train = train_df[:num_train]['is_duplicate'].values\n","            \n","X_test[:,0,:] = q1_feats[num_train:]\n","X_test[:,1,:] = q2_feats[num_train:]\n","Y_test = train_df[num_train:]['is_duplicate'].values\n","\n","# remove useless variables\n","del b\n","del q1_feats\n","del q2_feats"]},{"metadata":{"_cell_guid":"a6619eba-0fed-4cdc-8c67-8c631d61459c","collapsed":true,"_uuid":"d5a214ff7a18957560324b43014236df64a21cbf"},"cell_type":"code","outputs":[],"execution_count":null,"source":["from siamese import *\n","X_train[0]"]},{"metadata":{"_cell_guid":"bc0e1b7d-7717-4ff7-ab35-47dd5cad738d","collapsed":true,"_uuid":"c0fb82bf07d6459a0525783c8fac1d50c6163bb8"},"cell_type":"code","outputs":[],"execution_count":null,"source":["from __future__ import absolute_import\n","from __future__ import print_function\n","import numpy as np\n","\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, Dropout, Lambda, merge, BatchNormalization, Activation, Input, Merge\n","from keras import backend as K\n","from keras.layers import normalization\n","\n","\n","def euclidean_distance(vects):\n","    x, y = vects\n","    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n","\n","def eucl_dist_output_shape(shapes):\n","    shape1, shape2 = shapes\n","    return (shape1[0], 1)\n","\n","def cosine_distance(vests):\n","    x, y = vests\n","    x = K.l2_normalize(x, axis=-1)\n","    y = K.l2_normalize(y, axis=-1)\n","    return -K.mean(x * y, axis=-1, keepdims=True)\n","\n","def cos_dist_output_shape(shapes):\n","    shape1, shape2 = shapes\n","    return (shape1[0],1)\n","\n","def contrastive_loss(y_true, y_pred):\n","    '''Contrastive loss from Hadsell-et-al.'06\n","    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n","    '''\n","    margin = 1\n","    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n","\n","\n","def create_base_network(input_dim):\n","    '''\n","    Base network for feature extraction.\n","    '''\n","    input = Input(shape=(input_dim, ))\n","    # print(input)\n","    dense1 = Dense(128)(input)\n","    #bn1 = BatchNormalization(mode=2)(dense1)\n","    relu1 = Activation('relu')(dense1)\n","\n","    dense2 = Dense(128)(relu1)\n","    #bn2 = BatchNormalization(mode=2)(dense2)\n","    res2 = merge([relu1, dense2], mode='sum')\n","    relu2 = Activation('relu')(res2)    \n","\n","    dense3 = Dense(128)(relu2)\n","    #bn3 = BatchNormalization(mode=2)(dense3)\n","    res3 = Merge(mode='sum')([relu2, dense3])\n","    relu3 = Activation('relu')(res3)   \n","    \n","    feats = merge([relu3, relu2, relu1], mode='concat')\n","    # bn4 = BatchNormalization(mode=2)(feats)\n","\n","    model = Model(input=input, output=feats)\n","\n","    return model\n","\n","\n","def compute_accuracy(predictions, labels):\n","    '''\n","    Compute classification accuracy with a fixed threshold on distances.\n","    '''\n","    return labels[predictions.ravel() < 0.5].mean()\n","\n","def create_network(input_dim):\n","    # network definition\n","    base_network = create_base_network(input_dim)\n","    \n","    input_a = Input(shape=(input_dim,))\n","    input_b = Input(shape=(input_dim,))\n","    \n","    # because we re-use the same instance `base_network`,\n","    # the weights of the network\n","    # will be shared across the two branches\n","    processed_a = base_network(input_a)\n","    processed_b = base_network(input_b)\n","    \n","    distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n","    \n","    model = Model(input=[input_a, input_b], output=distance)\n","    return model"]},{"metadata":{"_cell_guid":"6ba3b6ad-1b9e-4064-a9f8-a3b7f4d9acda","collapsed":true,"_uuid":"d5f14c68dfc01ec7e947e9905887dd7b37989bec"},"cell_type":"code","outputs":[],"execution_count":null,"source":["# create model\n","from keras.optimizers import RMSprop, SGD, Adam\n","net = create_network(300)\n","\n","# train\n","#optimizer = SGD(lr=1, momentum=0.8, nesterov=True, decay=0.004)\n","optimizer = Adam(lr=0.001)\n","net.compile(loss=contrastive_loss, optimizer=optimizer)\n","\n","for epoch in range(50):\n","    net.fit([X_train[:,0,:], X_train[:,1,:]], Y_train,\n","          validation_data=([X_test[:,0,:], X_test[:,1,:]], Y_test),\n","          batch_size=128, nb_epoch=1, shuffle=True, )\n","    \n","    # compute final accuracy on training and test sets\n","    pred = net.predict([X_test[:,0,:], X_test[:,1,:]], batch_size=128)\n","    te_acc = compute_accuracy(pred, Y_test)\n","    \n","#    print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n","    print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))"]},{"metadata":{"_cell_guid":"88cd8611-abff-4a76-8ada-2fe517ec2555","collapsed":true,"_uuid":"e2c9f047dda1e93445c01b9f70fada1694684f9d"},"cell_type":"code","outputs":[],"execution_count":null,"source":[]},{"metadata":{"_cell_guid":"8a0ec195-2e8a-4033-b9ef-276e223fbdcf","collapsed":true,"_uuid":"3d31f99871147cf41b7c79a5805654cb07ac339f"},"cell_type":"code","outputs":[],"execution_count":null,"source":[]},{"metadata":{"_cell_guid":"2011b255-0456-423d-b920-5ef3de48d22f","collapsed":true,"_uuid":"66243b675a2f50d24cf9b8352255e86703bbf326"},"cell_type":"code","outputs":[],"execution_count":null,"source":[]},{"metadata":{"_cell_guid":"238452b8-60b7-46c0-9d4a-c423a9d63fde","collapsed":true,"_uuid":"e45b6b0f2b3eaabfa0e933649ed8376c9299abb2"},"cell_type":"code","outputs":[],"execution_count":null,"source":[]},{"metadata":{"_cell_guid":"f8a753a5-b672-45d0-acb7-377a18e95c9b","collapsed":true,"_uuid":"73f3de7ddcaed10dcbde26f097e4bf9d3a97100d"},"cell_type":"code","outputs":[],"execution_count":null,"source":[]},{"metadata":{"_cell_guid":"6cd1d219-60f4-4d78-8383-6298907193e5","collapsed":true,"_uuid":"207e8d6d88cd27f6f751471c97fefafc4108db76"},"cell_type":"code","outputs":[],"execution_count":null,"source":[]},{"metadata":{"_cell_guid":"dce5d9e7-5086-4aee-a833-aa3174e5689b","collapsed":true,"_uuid":"07b5c8c8bb0ebd50f4d5a1e18c6dc9a75aba83d5"},"cell_type":"code","outputs":[],"execution_count":null,"source":[]},{"metadata":{"_cell_guid":"9e9fdbef-de1d-43f3-9ae8-f9917adef24b","collapsed":true,"_uuid":"e6b5c99b831a5dac50d74de9bf8e1d559d5bd98a"},"cell_type":"code","outputs":[],"execution_count":null,"source":[]},{"metadata":{"_cell_guid":"c8326901-fb2c-495b-9f2b-950bfac6fe6d","collapsed":true,"_uuid":"95842cd4554f2515e1f60c0b6e81e5762b1e9c4b"},"cell_type":"code","outputs":[],"execution_count":null,"source":[]},{"metadata":{"_cell_guid":"b2b18fab-9fae-4890-8f9a-78b9eb224b1e","collapsed":true,"_uuid":"a3abcd5575d1e542752156efd950261bdb4c72ca"},"cell_type":"code","outputs":[],"execution_count":null,"source":[]},{"metadata":{"_cell_guid":"00538c43-3562-4bfd-892a-a01e00a590f9","collapsed":true,"_uuid":"029ff664f635895fe4f7511103a1a909389a76dc"},"cell_type":"code","outputs":[],"execution_count":null,"source":[]},{"metadata":{"_cell_guid":"aef911c1-46fe-43cb-b4f8-5e8a743bfb07","collapsed":true,"_uuid":"2b3e732e2d40223ff3d9f6fa27cc91e175ba872c"},"cell_type":"code","outputs":[],"execution_count":null,"source":[]},{"metadata":{"_cell_guid":"e1bf65c6-9135-4ac5-aa1a-ace6660127ff","collapsed":true,"_uuid":"1b0303bcbbc2c163235e8b3b24e6997d4d902cad"},"cell_type":"code","outputs":[],"execution_count":null,"source":[]},{"metadata":{"_cell_guid":"8078d24f-7b1a-4c35-8bca-49ceade9db79","collapsed":true,"_uuid":"9afd2ad3affa4b51915c0c3513d005f3e6966c61"},"cell_type":"code","outputs":[],"execution_count":null,"source":[]},{"metadata":{"_cell_guid":"6e8b144a-a874-463f-8f34-3ece9e871eae","collapsed":true,"_uuid":"72817d07e006b69c76809bd364c0698503d53f9a"},"cell_type":"code","outputs":[],"execution_count":null,"source":[]}],"metadata":{"language_info":{"mimetype":"text/x-python","version":"3.6.4","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","file_extension":".py","name":"python","pygments_lexer":"ipython3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1,"nbformat":4}